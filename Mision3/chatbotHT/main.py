# ==========================================================
# main.py â€” Chatbot con GPT + RAG + FAISS (2025)
# ==========================================================

from flask import Flask, render_template, request, jsonify
import os
import random

# Modelos previos (clusters del usuario)
from chatbot.data import training_data
from chatbot.model import build_and_train_model, load_model, predict_cluster

# Procesamiento de documentos
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# OpenAI SDK nuevo (2025)
from openai import OpenAI
client = OpenAI()

# Variables de entorno
from dotenv import load_dotenv
dotenv_path = os.path.join(os.path.dirname(__file__), ".env")
load_dotenv(dotenv_path, override=True)

VECTOR_PATH = "vector_db"

print("API KEY DETECTADA:", os.getenv("OPENAI_API_KEY"))


# ==========================================================
# ğŸ”§ Prueba automÃ¡tica de API
# ==========================================================
try:
    test = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Hola, Â¿funcionas?"}]
    )
    print("OpenAI funcionando â†’", test.choices[0].message.content)
except Exception as e:
    print("âŒ Error al probar OpenAI:", e)


# ==========================================================
# ğŸ“„ Cargar y vectorizar documentos
# ==========================================================
def procesar_documento(file_path):
    ext = file_path.split(".")[-1].lower()

    if ext == "pdf":
        loader = PyPDFLoader(file_path)
    elif ext == "txt":
        loader = TextLoader(file_path)
    elif ext == "docx":
        loader = Docx2txtLoader(file_path)
    else:
        return None, f"âŒ Tipo de archivo no soportado: {ext}"

    docs = loader.load()

    # Dividir en chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.split_documents(docs)

    # Crear embeddings y FAISS
    embeddings = OpenAIEmbeddings()
    vector_db = FAISS.from_documents(chunks, embeddings)

    # Guardar el vector store
    vector_db.save_local(VECTOR_PATH)

    return True, "Documento procesado correctamente."


# ==========================================================
# ğŸ¤– Modelo de clusters
# ==========================================================
app = Flask(__name__)
model, vectorizer = load_model()
if model is None:
    model, vectorizer = build_and_train_model(training_data, n_clusters=6)


RESPUESTAS = {
    0: ["Â¡Hola! ğŸ˜Š Â¿CÃ³mo estÃ¡s?", "Â¡QuÃ© gusto saludarte!", "Â¿En quÃ© puedo ayudarte hoy?"],
    1: ["Hasta luego ğŸ‘‹", "Nos vemos pronto.", "Â¡CuÃ­date! ğŸ˜Š"],
    2: ["Soy un asistente virtual creado para ayudarte ğŸ’»", "PregÃºntame lo que quieras ğŸ˜‰"],
    3: ["Â¡Claro! Â¿En quÃ© puedo ayudarte?", "CuÃ©ntame tu problema ğŸ¤–"],
    4: ["Â¡Gracias a ti! â¤ï¸", "Me alegra ser de ayuda ğŸ˜„"],
    5: ["Lamento eso ğŸ˜”, puedo intentarlo nuevamente.", "Parece que algo no saliÃ³ bien ğŸ˜…"],
}


# ==========================================================
# ğŸŒ Rutas Flask
# ==========================================================

@app.route("/")
def home():
    return render_template("index.html")


# --- Subir documento ---
@app.route("/upload", methods=["POST"])
def upload():
    if "file" not in request.files:
        return jsonify({"message": "âŒ No enviaste archivo"})

    file = request.files["file"]

    if file.filename == "":
        return jsonify({"message": "âŒ Nombre de archivo vacÃ­o"})

    # Guardar archivo en servidor
    path = os.path.join("uploads", file.filename)
    os.makedirs("uploads", exist_ok=True)
    file.save(path)

    ok, msg = procesar_documento(path)
    return jsonify({"message": msg})


# --- CHAT ---
@app.route("/chat", methods=["POST"])
def chat():
    user_text = request.form.get("message", "").strip()

    if not user_text:
        return jsonify({"response": "Por favor escribe algo ğŸ˜…"})

    # ==========================================================
    # 1ï¸âƒ£ RAG: Responder con GPT usando el documento como contexto
    # ==========================================================
    if os.path.exists(VECTOR_PATH):
        try:
            embeddings = OpenAIEmbeddings()
            vector_db = FAISS.load_local(
                VECTOR_PATH,
                embeddings,
                allow_dangerous_deserialization=True
            )
            retriever = vector_db.as_retriever(search_kwargs={"k": 3})

            # ğŸ”¥ Nueva forma en LangChain 2025
            docs = retriever.invoke(user_text)

            contexto = "\n\n".join([d.page_content for d in docs])

            # GPT responde usando el documento
            prompt = f"""
Eres un asistente experto. Responde usando ÃšNICAMENTE el siguiente contexto
proveniente del documento, sin inventar nada.

Si la respuesta no estÃ¡ en el contexto, dilo claramente.

--- CONTEXTO ---
{contexto}
----------------

Pregunta del usuario:
{user_text}
"""

            ai_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Eres un asistente Ãºtil y preciso."},
                    {"role": "user", "content": prompt}
                ]
            )

            respuesta_gpt = ai_response.choices[0].message.content
            return jsonify({"response": respuesta_gpt})

        except Exception as e:
            print("âš  Error en RAG:", e)

    # ==========================================================
    # 2ï¸âƒ£ Si no hay documento â†’ usar GPT normal
    # ==========================================================
    try:
        ai_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un asistente amable y Ãºtil."},
                {"role": "user", "content": user_text}
            ]
        )
        respuesta_gpt = ai_response.choices[0].message.content
        return jsonify({"response": respuesta_gpt})

    except Exception as e:
        print("âš  Error con OpenAI:", e)

    # ==========================================================
    # 3ï¸âƒ£ Backup: Modelo de ClÃºsters
    # ==========================================================
    cluster = predict_cluster(model, vectorizer, user_text)
    response = random.choice(
        RESPUESTAS.get(cluster, ["No estoy seguro de entender ğŸ˜…, pero puedo intentarlo otra vez."])
    )
    return jsonify({"response": response})


# ==========================================================
# ğŸš€ Ejecutar servidor
# ==========================================================
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
